# SmartCapex project Deployment Using Custom GCP Vertex AI Pipeline

The pipeline consists of the following components:

- Data Ingestion
- Data Preprocessing
- Technical pipeline
    - Traffic Forcastion
    - Traffic improvement
    - Upgrade Selection
- Technical to economical pipeline
- Economical pipeline

## Prerequisites

Before running the pipeline, the following prerequisites must be completed:

- **Select or create a GCP project:** Follow the instructions [here](https://developers.google.com/workspace/guides/create-project) to create a GCP project. Also, enable [billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project) for your project.

- **Activate Vertex AI on GCP:** To use Vertex AI, you will need to enable the Vertex AI API on your GCP account. Follow the instructions in the [official documentation](https://cloud.google.com/vertex-ai/docs/start) to activate Vertex AI.

- **Create a Google Storage Bucket:** You will need to create a Google Storage Bucket to store the data and artifacts generated by the pipeline. Follow the instructions in the [official documentation](https://cloud.google.com/storage/docs/creating-buckets) to create a bucket.


## Usage

To run the pipeline, follow these steps:

1. **Clone the repository** [to your GCP VM instance.](myref)
2. **Create a virtual Python environment:** You should create a virtual Python environment to run the code in this repository. Follow the instructions [here](https://virtualenv.pypa.io/en/stable/user_guide.html) to create a virtual environment using `virtualenv`.
3. **Activate the virtual environment:** Activate the virtual environment you created in Step 2.
```bash
source /path/to/venv/bin/activate
```
4. **Install the required packages in your activated virtual environment:**
```bash
pip install google-cloud-aiplatform
pip install kfp
```
5. **Select your virtual environment as your kernel in the Jupyter Notebook (pipeline.ipynb):** You can use [ipykernel](https://janakiev.com/blog/jupyter-virtual-envs/) to add the virtual environment to your notebook.
6. **Build custom container images:** Before you can run the notebook/pipeline, you would also need to build custom container images that will host the source code of the whole project.

Once all the steps above are complete, you should be able to run the Jupyter Notebook (`pipeline.ipynb`). When the pipeline has finished running, you can find all intermediate results corresponding to each bloc stored on BigQuery tables. 

## Code Structure

The repository has the following structure:

- `pipeline.ipynb`: The main Jupyter Notebook used to run the custom Vertex AI pipeline.
- `src`: A directory containing the following subdirectories,
  - `preprocessing`: Contains scripts for cleaning and preprocessing the raw data. 
  - `traffic_forcasting`: Contains script for training fbprophet models corresponding to each cell and each KPI. 
  - `upgrade_selection`: Contains script for 
  - `utils`: ...
- `requirements.txt`: Contains Python packages required to run the pipeline and will be used for creating a custom Docker container image.
- `Dockerfile`: Contains instructions for building the custom Docker container image.
- `sh_build_image.sh`: Shell script to build the custom training container image.
- `sh_push_image.sh`: Shell script to push the custom training container image to Google Artifact Registry.

## Ressources

* [How to configure a VCP network](./Docs/How_to_configure_vcp_network.md)
* [How to Automatic Stopping of Notebook Instance using Cloud Scheduler](./Docs/How_to_Automatic_stopping_instance.md)
* 